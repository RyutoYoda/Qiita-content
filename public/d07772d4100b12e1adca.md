---
title: ' 正則化と過学習: L1正則化とL2正則化の役割'
tags:
  - 機械学習
  - 正則化
private: false
updated_at: '2024-06-12T11:25:18+09:00'
id: d07772d4100b12e1adca
organization_url_name: null
slide: false
ignorePublish: false
---
## はじめに
機械学習モデルの性能を高めるために、訓練データに対する精度を追求しすぎると、過学習（オーバーフィッティング）が発生する可能性があります。過学習は、モデルが訓練データに対しては高い精度を示すものの、未知のデータに対してはうまく一般化できない状態を指します。この問題を解決するために、正則化（レギュラライゼーション）という技術が用いられます。本記事では、過学習とその対策としてのL1正則化およびL2正則化について詳しく説明します。

## 過学習とは
過学習は、モデルが訓練データのノイズや詳細に過度に適応してしまい、新しいデータに対しての予測性能が低下する現象です。過学習が起こる原因としては、以下のような要因が考えられます。

- 複雑なモデルを使用している
- 訓練データが少ない
- 訓練データにノイズが多い

過学習を防ぐためには、適切なモデルの選択、データの増加、そして正則化が重要です。

## 正則化とは
正則化は、モデルの複雑さを抑え、過学習を防ぐための技術です。正則化にはいくつかの種類がありますが、代表的なものとしてL1正則化（Lasso）とL2正則化（Ridge）が挙げられます。

## L1正則化（Lasso）
L1正則化は、モデルの重みの絶対値の和にペナルティを課す手法です。L1正則化の目的は、重みの一部をゼロにすることによって特徴選択を行い、モデルをスパースにすることです。これにより、モデルの複雑さが減少し、過学習を防ぎます。L1正則化の数式は以下のようになります。

```math
\text{Loss} = \text{Loss}_{\text{original}} + \lambda \sum_{i} |w_i|
```

ここで、\(\lambda\)は正則化の強さを制御するハイパーパラメータです。

#### L1正則化の利点
- 特徴選択が自動的に行われる
- スパースモデルが得られる

## L2正則化（Ridge）
L2正則化は、モデルの重みの二乗和にペナルティを課す手法です。L2正則化は、重みが大きくなりすぎないようにすることで、モデルの複雑さを抑えます。L2正則化の数式は以下のようになります。

```math
\text{Loss} = \text{Loss}_{\text{original}} + \lambda \sum_{i} w_i^2
```

#### L2正則化の利点
- 過学習の防止
- すべての特徴が均等に扱われる

## L1正則化とL2正則化の違い
L1正則化とL2正則化はどちらも過学習を防ぐための有効な手段ですが、その動作原理や結果には違いがあります「L1正則化」は**特徴選択**を行うため、モデルをシンプルに保ちます。一方、「L2正則化」は**すべての特徴を均等に扱う**ため、特定の重みが大きくなるのを防ぎます。いずれにせよ過学習を抑制するためには**正則化パラメータを増やす**必要があると言えます。

### 実装例
L1正則化およびL2正則化は、多くの機械学習ライブラリでサポートされています。以下に、PythonのScikit-learnライブラリを使用した回帰モデルにおける実装例を示します。
```
from sklearn.linear_model import Lasso, Ridge
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

# データの準備
X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# L1正則化 (Lasso) 回帰
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
lasso_pred = lasso.predict(X_test)

# L2正則化 (Ridge) 回帰
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)
ridge_pred = ridge.predict(X_test)
```

## 結論
正則化は、過学習を防ぎ、モデルの一般化性能を向上させるための重要な手法です。L1正則化とL2正則化は、それぞれ異なる方法でモデルの複雑さを抑えます。L1正則化は特徴選択を行うのに対し、L2正則化は重みの大きさを抑えます。適切な正則化手法を選択することで、より良いモデルを構築することができます。

今後のモデル開発において、正則化の理解と適用は重要なスキルとなります。是非、この技術を活用して、より効果的な機械学習モデルを構築してください。
