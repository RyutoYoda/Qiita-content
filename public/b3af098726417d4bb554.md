---
title: オンプレミスのHadoopシステムをCloud Dataprocに移行する方法
tags:
  - hadoop
  - SQL
  - Cloud
  - Dataproc
  - GoogleCloud
private: false
updated_at: '2024-06-22T14:14:51+09:00'
id: b3af098726417d4bb554
organization_url_name: null
slide: false
ignorePublish: false
---
## はじめに 
この記事では、オンプレミスのHadoopシステムをGoogle CloudのCloud Dataprocに移行する手順について解説します。特に、Hiveを主要なツールとして使用し、Optimized Row Columnar（ORC）形式のデータをCloud Storageに保存し、パフォーマンスを最大化するための方法を紹介します。

## 背景

オンプレミスのHadoopシステムをクラウドに移行することで、スケーラビリティの向上、コスト削減、運用管理の簡素化が期待できます。Cloud DataprocはGoogle CloudのフルマネージドApache HadoopおよびApache Sparkサービスであり、簡単にクラウド上で大規模データ処理を実行できます。

## 基本的な概念とツール

### Cloud Dataproc
Google CloudのフルマネージドなHadoopおよびSparkサービス。クラスターのセットアップ、管理、スケーリングが簡単です。

### HDFS（Hadoop Distributed File System）
分散ファイルシステムで、大量のデータを効率的に保存し、処理できます。

### Hive
Hadoopエコシステム内のデータウェアハウスツールで、SQLライクなクエリ言語を提供し、大量のデータを簡単にクエリできます。

### ORC（Optimized Row Columnar）
Hadoop用の効率的な列指向フォーマットで、パフォーマンスを向上させるために最適化されています。

### gsutil
Google Cloud Storageとやり取りするためのコマンドラインツール。データのアップロード、ダウンロード、コピーが可能です。

### Cloud Storage Connector for Hadoop
Google Cloud StorageとHadoopの間でデータ転送を容易にするツール。HadoopエコシステムをCloud Storageとシームレスに統合します。

### 外部Hiveテーブル
外部データソース上のデータを参照するHiveテーブル。データはテーブルが削除されても保持されます。

## データ移行とHiveの使用方法

### 1. ORCファイルのCloud Storageへのコピー

まず、オンプレミスのHadoopシステムからすべてのORCファイルをGoogle Cloud Storageにコピーします。これは既に完了しているものとします。

### 2. データをHDFSに複製

Cloud DataprocクラスターのローカルHDFSに一部のデータを複製することで、パフォーマンスを最大化できます。

#### 方法1: gsutilを使用してデータを転送

1. gsutilを使用して、Cloud StorageバケットからDataprocクラスターのHDFSにORCファイルを転送します。

```bash
# Dataprocクラスターの任意のノードに転送
gsutil cp gs://my-bucket/*.orc /path/to/local/directory

# HDFSにコピー
hadoop fs -put /path/to/local/directory/*.orc /path/to/hdfs/directory
```

2. HiveテーブルをローカルHDFSにマウントします。

```sql
CREATE EXTERNAL TABLE my_table (
    column1 STRING,
    column2 INT,
    ...
)
STORED AS ORC
LOCATION '/path/to/hdfs/directory';
```

#### 方法2: Cloud Storage Connector for Hadoopを使用

1. Cloud Storage Connector for Hadoopを使用して、ORCファイルを外部Hiveテーブルとしてマウントします。

```sql
CREATE EXTERNAL TABLE my_external_table (
    column1 STRING,
    column2 INT,
    ...
)
STORED AS ORC
LOCATION 'gs://my-bucket/';
```

2. 外部Hiveテーブルをネイティブテーブルに複製します。

```sql
CREATE TABLE my_native_table AS
SELECT * FROM my_external_table;
```

## 結論

オンプレミスのHadoopシステムをCloud Dataprocに移行する際、以下の2つの方法でHiveを使用することが可能です：

1. **gsutilを使用してデータを転送し、ローカルHDFSにマウント**：
   - gsutilを使ってCloud StorageからDataprocクラスターにORCファイルを転送し、HDFSにコピー。
   - HiveテーブルをローカルHDFSにマウント。

2. **Cloud Storage Connector for Hadoopを使用**：
   - Cloud Storage Connector for Hadoopを使ってORCファイルを外部Hiveテーブルとしてマウント。
   - 外部Hiveテーブルをネイティブテーブルに複製。

これらの方法を活用することで、Cloud Dataproc上でのHiveの使用を効率化し、パフォーマンスを最大化できます。プロジェクトの要件に合わせて最適な方法を選択してください。
